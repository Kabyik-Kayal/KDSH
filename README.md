# ðŸ‰ KDSH: Narrative Consistency Detection with Dragon Hatchling Architecture

> **Can a biologically-inspired neural network detect when a character's backstory contradicts a 19th-century novel?**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.6.1+ee4c2c.svg)](https://pytorch.org/)
[![Pathway](https://img.shields.io/badge/Pathway-0.27+-green.svg)](https://pathway.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

**KDSH** (Knowledge Detection with Sparse Hebbian networks) is a novel approach to narrative consistency detection that combines biologically-inspired neural architecture with modern NLP techniques. This project implements **TextPath**, a custom language model built on the [Dragon Hatchling (BDH)](https://arxiv.org/abs/2509.26507) architecture, enhanced with entity threading and perplexity-based reasoning to catch subtle contradictions in literary character backstories.

---

## ðŸ“š What This Project Does

Imagine you're given a backstory about Edmond DantÃ¨s from *The Count of Monte Cristo*. Is it consistent with the novel, or does it contradict established facts? This system automatically detects such inconsistencies by:

1. **Understanding the source material** through novel-specific language model pretraining
2. **Extracting character narratives** using entity threading to create continuous character arcs
3. **Measuring consistency** via perplexity delta â€” does the backstory help or hinder the model's predictions?
4. **Making calibrated predictions** using logistic regression on learned features

### The Dataset

- **Two classic novels**: *The Count of Monte Cristo* (Alexandre Dumas) and *In Search of the Castaways* (Jules Verne)
- **80 training examples**: Character backstories labeled as consistent or contradictory
- **Novel statistics**:
  - *The Count of Monte Cristo*: 61,676 lines, 13 main characters
  - *In Search of the Castaways*: 18,728 lines, 12 main characters
- **Challenge**: Minimal training data, massive context (entire novels), subtle contradictions

---

## ðŸ§  The Architecture: Why BDH?

Traditional transformers struggle with this task because they need massive training data. We chose the **Dragon Hatchling (BDH)** architecture for its unique biological properties:

### Core BDH Properties

| Feature | What It Does | Why It Matters |
|---------|--------------|----------------|
| **Hebbian Learning** | "Neurons that fire together, wire together" | Naturally learns character relationships and narrative patterns from sequential text |
| **Sparse Activations** | Only ~5% of neurons fire per input | Creates interpretable, monosemantic representations where each concept has distinct neural signatures |
| **Scale-Free Networks** | Power-law connectivity like biological brains | Efficient information routing with fewer parameters |
| **Dynamic Synapses** | Edge weights update during inference | Builds context-specific working memory across long narratives |

### TextPath: BDH for Text

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TextPath Architecture                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  Input Tokens [batch, seq_len]                             â”‚
â”‚      â†“                                                     â”‚
â”‚  Token Embedding (16K vocab â†’ 256D)                        â”‚
â”‚      â†“                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ BDH Layer Ã— 4                                        â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ 1. Project to neurons: v â†’ x (ReLU + Sparse)  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ 2. Multi-head attention: x Ã— x â†’ a           â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ 3. Hebbian update: y = (a Â· Dy) âŠ™ x          â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ 4. Residual: v â† v + y Â· E                   â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚      â†“                                                     â”‚
â”‚  Language Model Head â†’ Next Token Predictions              â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Parameters**: ~8M (compared to 100M+ for equivalent transformers)
â”‚  â”‚   â””â”€ RoPE Positional Encoding (max_seq_len=4096)               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â†“                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Classification Mode (when enabled):                            â”‚  â”‚
â”‚  â”‚   LayerNorm â†’ Dropout â†’ Linear(256â†’128) â†’ GELU                 â”‚  â”‚
â”‚  â”‚   â†’ Dropout â†’ Linear(128â†’2) â†’ [Contradict, Consistent]         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                      â”‚
â”‚  Output: Logits [batch_size, 2] for classification                   â”‚
â”‚          OR Logits [batch_size, seq_len, vocab_size] for LM          â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### BDH Biological Properties

The Dragon Hatchling architecture provides three key advantages for narrative understanding:

#### 1. Hebbian Learning
> *"Neurons that fire together, wire together"*

Pre-training on sequential novel passages builds causal circuits encoding:
- Character relationships (DantÃ¨s â†’ MercÃ©dÃ¨s, Fernand â†’ betrayal)
- Plot events (imprisonment â†’ escape â†’ revenge)
- Narrative logic (foreshadowing â†’ resolution)

---

## ðŸŽ¯ The Key Innovation: Perplexity Delta Scoring

Instead of using a traditional classification head, we use a **generative reasoning** approach:

### The Intuition

If a backstory is **consistent** with a novel, then conditioning the language model on that backstory should **reduce** perplexity when predicting novel passages.

$$\Delta = \text{Loss}(\text{novel} \mid \emptyset) - \text{Loss}(\text{novel} \mid \text{backstory})$$

- **Positive Î”**: Backstory helps predict novel â†’ **CONSISTENT**
- **Negative Î”**: Backstory hurts prediction â†’ **CONTRADICTORY**
- **Near-zero Î”**: Backstory is neutral â†’ **AMBIGUOUS**

### Why This Works

1. **Information-theoretic**: Perplexity measures how "surprised" the model is
2. **Calibrated**: We train a logistic regression on `[Î”, cosine_similarity]` features
3. **Interpretable**: Delta scores show *how much* the model believes the backstory

### Entity Threading: Learning Character Arcs

Standard chunk-based pretraining breaks narrative continuity. We solve this with **entity threading**:

```python
# Extract all paragraphs mentioning "DantÃ¨s" from Monte Cristo
thread_dantÃ¨s = extract_character_paragraphs(novel, ["DantÃ¨s", "Edmond"])
# â†’ Creates continuous narrative: sailor â†’ betrayal â†’ prison â†’ escape â†’ count
```

This forces the BDH model to learn:
- Long-range character development
- Relationship dynamics  
- Plot-critical events
- Narrative causality

**Result**: The model develops a coherent "memory" of each character's story arc.

---

## ðŸ“‚ Project Structure

```
KDSH/
â”œâ”€â”€ run_pipeline.py                    # Main CLI entry point
â”œâ”€â”€ requirements.txt                   # Dependencies
â”œâ”€â”€ results.csv                        # Final predictions for submission
â”œâ”€â”€ LICENSE                            # MIT License
â”œâ”€â”€ README.md                          # Project documentation
â”‚
â”œâ”€â”€ Dataset/
â”‚   â”œâ”€â”€ train.csv                      # 80 labeled training pairs
â”‚   â”œâ”€â”€ test.csv                       # Unlabeled test set
â”‚   â”œâ”€â”€ Books/
â”‚   â”‚   â”œâ”€â”€ The Count of Monte Cristo.txt
â”‚   â”‚   â””â”€â”€ In search of the castaways.txt
â”‚   â””â”€â”€ entity_threads/                # Character-specific narrative threads
â”‚       â”œâ”€â”€ The Count of Monte Cristo/
â”‚       â”‚   â”œâ”€â”€ thread_dantÃ¨s.txt      # DantÃ¨s' complete arc
â”‚       â”‚   â””â”€â”€ ... (13 characters)
â”‚       â””â”€â”€ In search of the castaways/
â”‚           â”œâ”€â”€ thread_paganel.txt
â”‚           â””â”€â”€ ... (12 characters)
â”‚
â”œâ”€â”€ models/                            # Trained model checkpoints
â”‚   â”œâ”€â”€ custom_tokenizer.json          # 16,384 vocab BPE tokenizer
â”‚   â”œâ”€â”€ textpath_the_count_of_monte_cristo.pt
â”‚   â”œâ”€â”€ textpath_in_search_of_the_castaways.pt
â”‚   â””â”€â”€ calibration_model.pkl          # Logistic regression calibrator
â”‚
â”œâ”€â”€ src/                               # Source code modules
â”‚   â”œâ”€â”€ config.py                      # Pipeline configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ analysis/                      # Scoring modules
â”‚   â”‚   â””â”€â”€ consistency_scorer.py      # Perplexity delta scorer
â”‚   â”‚
â”‚   â”œâ”€â”€ data_processing/               # Data and RAG
â”‚   â”‚   â”œâ”€â”€ build_retrievers.py        # Retriever factory
â”‚   â”‚   â”œâ”€â”€ classification_dataset.py  # PyTorch Dataset
â”‚   â”‚   â”œâ”€â”€ entity_threading.py        # Character thread extraction
â”‚   â”‚   â”œâ”€â”€ retrieval.py               # PathwayNovelRetriever
â”‚   â”‚   â””â”€â”€ train_tokenizer.py         # Custom BPE tokenizer training
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/                    # Evaluation and prediction
â”‚   â”‚   â””â”€â”€ evaluate.py                # Metrics and prediction
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                        # Neural network modules
â”‚   â”‚   â”œâ”€â”€ pretrain_bdh_native.py     # Hebbian pretraining
â”‚   â”‚   â””â”€â”€ textpath.py                # TextPath/BDH core architecture
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                      # Training infrastructure
â”‚   â”‚   â”œâ”€â”€ calibration.py             # Logistic regression calibration
â”‚   â”‚   â””â”€â”€ pretraining.py             # Pretraining runner
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                         # Helper functions
â”‚   â”‚   â””â”€â”€ seed.py                    # Reproducibility seeds
â”‚   â”‚
â”‚   â””â”€â”€ visualization/                 # Analysis and plots
â”‚       â””â”€â”€ visualize.py               # Visualization functions
â”‚
â”œâ”€â”€ visualizations/                    # Generated plots
â”œâ”€â”€ repos/                             # External dependencies
â”‚   â””â”€â”€ bdh_educational/               # Educational BDH implementation
â”‚       â””â”€â”€ bdh.py                     # Core BDH module
â”‚
â”œâ”€â”€ outputs/                           # Training artifacts
â”‚   â””â”€â”€ optimal_config.json            # Best hyperparameters
â”‚
â””â”€â”€ logs/                              # Training logs
```

## ðŸ”§ Installation

### Prerequisites
- Python 3.11+
- conda (recommended) or pip
- ~8GB RAM (for embedding models)

### Setup

```bash
# Clone the repository
git clone https://github.com/kabyik-kayal/kdsh.git
cd kdsh

# Create conda environment
conda create -n kds python=3.11
conda activate kds

# Install dependencies
pip install -r requirements.txt
```

### Key Dependencies

| Package | Purpose |
|---------|----------|
| `torch` | Deep learning framework |
| `pathway` | RAG document indexing |
| `sentence-transformers` | Embedding model for retrieval |
| `tokenizers` | BPE tokenizer |
| `scikit-learn` | Metrics and evaluation |
| `pandas` | Data manipulation |
| `matplotlib` | Visualization |
| `tqdm` | Progress bars |

---

## ðŸš€ Usage

### Quick Start: Complete Pipeline

```bash
#This pretrains the BDH models on both novels with entity threading (50 epochs, ~2 hours on GPU)
python run_pipeline.py --mode pretrain 

#This will run the entire pipeline from classification model training to prediction (EXCLUDING Pretraining)
python run_pipeline.py --mode full 
```

This will:
1. **Pretrain** BDH models on both novels with entity threading (50 epochs, ~2 hours on GPU)
2. **Train** calibration model using perplexity delta features (~5 minutes)
3. **Evaluate** on validation split and print metrics
4. **Predict** on test set and save to `results.csv`
5. **Visualize** results and save plots to `visualizations/`

### Pipeline Modes

The pipeline has 5 modes for different stages:

| Mode | What It Does | When to Use |
|------|--------------|-------------|
| **`pretrain`** | Train BDH language models on novels + entity threads | First run, or to retrain models |
| **`train`** | Train logistic calibration model on delta features | After pretraining, or with new hyperparameters |
| **`evaluate`** | Compute metrics on validation split | Check model performance |
| **`predict`** | Generate predictions for test.csv | Create submission file |
| **`visualize`** | Create all analysis plots | Analyze model behavior |
| **`full`** | Run all modes in sequence | Complete end-to-end run |

### Individual Mode Examples

```bash
# 1. Pretrain BDH models (do this first!)
python run_pipeline.py --mode pretrain --pretrain-epochs 50

# 2. Train calibration model
python run_pipeline.py --mode train

# 3. Evaluate on validation set
python run_pipeline.py --mode evaluate

# 4. Generate test predictions
python run_pipeline.py --mode predict

# 5. Create visualizations
python run_pipeline.py --mode visualize
```

### Command-Line Options

```bash
python run_pipeline.py --help

Usage: run_pipeline.py [OPTIONS]

Options:
  --mode {pretrain,train,predict,evaluate,visualize,full}
                        Pipeline mode (default: full)
  --pretrain-epochs INT
                        BDH pretraining epochs (default: 50)
  --epochs INT          Calibration training epochs (default: 15)
  --batch-size INT      Training batch size (default: 4)
  --lr FLOAT            Learning rate (default: 1e-4)
  --device {cuda,mps,cpu}
                        Device to use (default: auto-detect)
```

### Advanced Examples

```bash
# Longer pretraining for better narrative understanding
python run_pipeline.py --mode pretrain --pretrain-epochs 100

# Faster training with larger batches (requires more RAM)
python run_pipeline.py --mode train --batch-size 16 --lr 2e-4

# Force CPU usage (if GPU has issues)
python run_pipeline.py --mode full --device cpu

# Quick evaluation without retraining
python run_pipeline.py --mode evaluate
```

### What Gets Generated

After running, you'll find:

```
KDSH/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ textpath_the_count_of_monte_cristo.pt  # Pretrained BDH (50MB)
â”‚   â”œâ”€â”€ textpath_in_search_of_the_castaways.pt # Pretrained BDH (50MB)
â”‚   â””â”€â”€ calibration_model.pkl                   # Logistic regression (~100KB)
â”‚
â”œâ”€â”€ results.csv                                 # Test predictions (ready for submission)
â”‚
â”œâ”€â”€ visualizations/                             # Analysis plots
â”‚   â”œâ”€â”€ delta_distribution.png
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”œâ”€â”€ calibration_curve.png
â”‚   â”œâ”€â”€ feature_importance.png
â”‚   â””â”€â”€ evaluation_dashboard.png
â”‚
â””â”€â”€ outputs/
    â””â”€â”€ optimal_config.json                     # Best hyperparameters found
```

---

## âš™ï¸ Configuration

All settings live in [src/config.py](src/config.py) as the `PipelineConfig` dataclass. You can modify defaults directly in the file or override via command-line arguments.

### Key Configuration Groups

#### ðŸ“ Paths Configuration

```python
novels_dir = ROOT / 'Dataset' / 'Books'          # Novel .txt files
train_csv = ROOT / 'Dataset' / 'train.csv'       # Training labels
test_csv = ROOT / 'Dataset' / 'test.csv'         # Test set (no labels)
tokenizer_path = ROOT / 'models' / 'custom_tokenizer.json'
models_dir = ROOT / 'models'                     # Checkpoint directory
output_predictions = ROOT / 'results.csv'        # Final predictions
```

#### ðŸŽ›ï¸ Model Architecture

```python
# TextPath/BDH configuration
vocab_size = 16384          # BPE tokenizer vocabulary
max_seq_len = 512           # Maximum tokens per sequence
n_heads = 8                 # Multi-head attention
n_neurons = 2048            # BDH neurons (scale-free graph)
d_model = 256               # Embedding dimension
n_layers = 4                # Number of BDH layers
sparsity_target = 0.05      # 5% activation rate
```

#### ðŸ‹ï¸ Training Hyperparameters

```python
batch_size = 4              # Training batch size
epochs = 15                 # Calibration training epochs
learning_rate = 1e-4        # Initial learning rate
weight_decay = 0.01         # AdamW regularization
pretrain_epochs = 50        # BDH pretraining epochs
```

#### ðŸ” RAG (Retrieval) Settings

```python
chunk_size = 200            # Words per chunk (~250 tokens)
overlap = 50                # Overlapping words between chunks
top_k_retrieval = 2         # Number of passages to retrieve per query
```

#### ðŸŽ¯ Device Selection

```python
# Auto-detected in order: CUDA â†’ MPS (Apple Silicon) â†’ CPU
device = 'cuda' if torch.cuda.is_available() 
         else 'mps' if torch.backends.mps.is_available()
         else 'cpu'
```

### Modifying Configuration

**Option 1: Edit directly**

```python
# In src/config.py
@dataclass
class PipelineConfig:
    pretrain_epochs: int = 100  # Changed from 50
    batch_size: int = 8         # Changed from 4
```

**Option 2: Command-line override**

```bash
python run_pipeline.py --pretrain-epochs 100 --batch-size 8
```

**Option 3: Programmatic override**

```python
from src.config import get_config

config = get_config()
config.pretrain_epochs = 100
config.batch_size = 8
```

---

## ðŸ“Š How It Works: The Complete Pipeline

### Stage 1: Entity Threading (Pretraining Prep)

Extract character-specific narratives from novels:

```python
# From entity_threading.py
def create_character_threads(novel_path, character_list):
    """
    Extract all paragraphs mentioning each character.
    Creates continuous narrative threads preserving character arcs.
    """
    paragraphs = split_into_paragraphs(novel_text)
    
    for character, aliases in character_list:
        # Find all paragraphs mentioning this character
        thread = [p for p in paragraphs if any(alias in p for alias in aliases)]
        # Save as continuous text
        save_thread(f"thread_{character}.txt", thread)
```

**Output**: 25 character threads (13 for Monte Cristo, 12 for Castaways)

### Stage 2: BDH Pretraining

Train novel-specific language models on mixed data:

```python
# From pretrain_bdh_native.py
def pretrain_bdh_novel(novel_name):
    """
    Pretrain BDH on:
    - 70% raw novel text (narrative structure)
    - 30% entity threads (character-specific sequences)
    """
    model = TextPath(config)  # BDH architecture
    
    for epoch in range(50):
        for batch in mixed_data_loader:
            logits = model(input_ids)
            loss = cross_entropy(logits, targets)  # Next-token prediction
            loss.backward()
            optimizer.step()
```

**Output**: Two pretrained models (~50MB each)
- `textpath_the_count_of_monte_cristo.pt`
- `textpath_in_search_of_the_castaways.pt`

### Stage 3: Perplexity Delta Computation

For each training sample, compute consistency score:

```python
# From consistency_scorer.py
class ConsistencyScorer:
    def compute_delta(self, backstory, novel_chunk):
        """
        Delta = Loss(chunk | empty) - Loss(chunk | backstory)
        """
        # Baseline: predict chunk without context
        loss_baseline = model.compute_loss(novel_chunk)
        
        # Conditioned: predict chunk given backstory
        loss_conditioned = model.compute_loss(
            context=backstory, 
            target=novel_chunk
        )
        
        delta = loss_baseline - loss_conditioned
        # Positive delta = backstory helps = CONSISTENT
        # Negative delta = backstory hurts = CONTRADICTORY
        
        return delta, cosine_similarity(backstory_emb, chunk_emb)
```

### Stage 4: Calibration Training

Train logistic regression on delta features:

```python
# From calibration.py
def train_calibration_model(train_csv):
    """
    Train simple classifier on extracted features:
    - Feature 1: Perplexity delta
    - Feature 2: Cosine similarity (retrieval score)
    """
    features = []
    labels = []
    
    for sample in train_csv:
        # Retrieve relevant passages
        chunks = retriever.search(sample.content, k=2)
        
        # Compute delta
        delta, cos_sim = scorer.compute_delta(sample.content, chunks)
        features.append([delta, cos_sim])
        labels.append(sample.label)
    
    # Train calibrated classifier
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', LogisticRegression())
    ])
    pipeline.fit(features, labels)
    return pipeline
```

**Output**: `calibration_model.pkl` (~100KB)

### Stage 5: Prediction

Generate predictions for test set:

```python
# From evaluate.py
def run_prediction(test_csv, calibration_model):
    predictions = []
    
    for sample in test_csv:
        # 1. Retrieve relevant passages
        chunks = retriever.search(sample.content, k=2)
        
        # 2. Compute delta features
        delta, cos_sim = scorer.compute_delta(sample.content, chunks)
        
        # 3. Predict using calibration model
        prob = calibration_model.predict_proba([[delta, cos_sim]])
        label = 1 if prob[1] > 0.5 else 0  # 1=consistent, 0=contradict
        
        predictions.append({
            'id': sample.id,
            'label': label
        })
    
    save_predictions('results.csv', predictions)
```

**Output**: `results.csv` (ready for submission)

---

## ðŸ”¬ Technical Deep Dive

### Why Perplexity Delta Works

Perplexity measures how "surprised" a language model is by a sequence. Lower perplexity = better prediction.

**Intuition**: If a backstory is consistent with a novel, it should *reduce* the model's surprise when reading novel passages.

$$\text{Perplexity}(x) = \exp\left(-\frac{1}{N}\sum_{i=1}^N \log P(x_i | x_{<i})\right)$$

$$\Delta = \text{PPL}(x | \emptyset) - \text{PPL}(x | \text{backstory})$$

**Example**:

```
Novel passage: "DantÃ¨s was arrested on his wedding day and imprisoned in ChÃ¢teau d'If."

Consistent backstory: "DantÃ¨s was betrayed by jealous rivals who framed him for treason."
â†’ Model thinks: "Oh, that makes sense! Betrayal â†’ arrest â†’ prison"
â†’ Perplexity decreases â†’ Positive Î” â†’ CONSISTENT

Contradictory backstory: "DantÃ¨s was a wealthy count who lived in Paris all his life."
â†’ Model thinks: "Wait, that doesn't match. How was he arrested at a wedding?"
â†’ Perplexity increases â†’ Negative Î” â†’ CONTRADICTORY
```

### BDH Architecture Details

The Dragon Hatchling uses **Hebbian learning principles**:

```python
# From bdh.py (simplified)
class BDH(nn.Module):
    def forward(self, input_ids):
        v_ast = self.emb(input_ids)  # Token embeddings
        
        for layer in range(self.L):
            # 1. Sparse neuron activation (ReLU â†’ ~5% active)
            x = F.relu(v_ast @ self.Dx)  # Project to neurons
            
            # 2. Hebbian attention (neurons communicate)
            a_ast = self.linear_attn(x, x, v_ast)  # x Ã— x^T Ã— v
            
            # 3. Synaptic strengthening (co-firing â†’ stronger weights)
            y = F.relu(a_ast @ self.Dy) * x  # Hadamard product
            
            # 4. Residual update (gradual memory accumulation)
            v_ast = v_ast + y @ self.E
        
        return v_ast @ self.readout  # Next-token logits
```

**Key differences from Transformers**:

| Feature | Transformer | BDH |
|---------|-------------|-----|
| **Connectivity** | Dense (all-to-all) | Sparse (scale-free graph) |
| **Activation** | ~100% neurons | ~5% neurons (sparse) |
| **Learning** | Gradient descent only | Hebbian + gradient descent |
| **Parameters** | O(dÂ²) | O(dÂ·âˆšN) (low-rank) |
| **Interpretability** | Polysemantic | Monosemantic neurons |

### Pathway RAG Integration

Satisfies Track B requirement for Pathway framework:

```python
# From retrieval.py
class PathwayNovelRetriever:
    def __init__(self, novel_path):
        # Create Pathway table (required for Track B)
        self.chunks_table = pw.debug.table_from_rows(
            schema=pw.schema_from_dict({"text": str}),
            rows=[(chunk,) for chunk in self.chunks]
        )
        
        # Use Pathway's embedding framework
        from pathway.xpacks import llm
        self.embedder = llm.embedders.SentenceTransformerEmbedder(
            model="sentence-transformers/all-MiniLM-L6-v2"
        )
        
        # Build vector index
        self.embeddings = self.chunks_table.select(
            embedding=self.embedder(pw.this.text)
        )
    
    def search(self, query, k=2):
        """Retrieve top-k most relevant chunks"""
        query_emb = self.embedder.encode([query])[0]
        scores = cosine_similarity(query_emb, self.embeddings)
        top_k_idx = np.argsort(scores)[-k:]
        return [self.chunks[i] for i in top_k_idx]
```

### Model Training Details

**Pretraining** (Unsupervised Language Modeling):
- **Objective**: Next-token prediction
- **Data**: 70% raw novel + 30% entity threads
- **Batch size**: 16 sequences Ã— 512 tokens
- **Optimizer**: AdamW (lr=3e-4, weight_decay=0.01)
- **Duration**: 50 epochs (~60 mins on  H200)
- **Loss**: Cross-entropy on vocabulary

**Calibration** (Supervised Classification):
- **Objective**: Binary classification on delta features
- **Data**: 80 training samples â†’ [delta, cos_sim] features
- **Model**: Logistic Regression with StandardScaler
- **Cross-validation**: 5-fold CV for robustness
- **Duration**: <1 minute on CPU
- **Metrics**: Accuracy, F1, Precision, Recall

## ðŸ“Š Visualizations & Analysis

The pipeline generates comprehensive analysis plots:

### Delta Distribution Plot
Shows the separation between consistent and contradictory samples based on perplexity delta:

```python
# From visualize.py
plot_delta_distribution(
    deltas=[...],          # Delta values for all samples
    labels=[...],          # True labels (0=contradict, 1=consistent)
    novel_names=[...]      # Novel for each sample
)
```

**What to look for**: Clear separation between red (contradict) and green (consistent) distributions

### Confusion Matrix
Standard evaluation visualization:

```python
plot_confusion_matrix(y_true, y_pred)
```

Shows: True Positives, False Positives, True Negatives, False Negatives

### Calibration Curve
Checks if predicted probabilities match actual frequencies:

```python
plot_calibration_curve(y_true, y_probs)
```

**Interpretation**: Diagonal line = perfectly calibrated model

### Feature Importance
Shows relative importance of delta vs. cosine similarity:

```python
plot_feature_importance(calibration_model.coef_)
```

### Generate All Visualizations

```bash
python run_pipeline.py --mode visualize
```

Saves plots to `visualizations/` directory.

---

## ðŸŽ“ Key Insights & Lessons Learned

### What Works

âœ… **Entity Threading**: Massive improvement over random chunking. Character-specific sequences help the model learn narrative arcs.

âœ… **Perplexity Delta**: More principled than classification head. Information-theoretic measure naturally captures consistency.

âœ… **Novel-Specific Models**: Each novel has unique style, vocabulary, and themes. Separate models perform better than one shared model.

âœ… **BDH Sparse Activations**: ~5% activation rate creates interpretable representations. Makes debugging easier.

âœ… **Pathway Integration**: Clean API for document indexing. Sentence-transformers embedder works well out of the box.