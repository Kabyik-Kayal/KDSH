# ğŸ‰ KDSH 2026: Narrative Consistency Detection with Dragon Hatchling Architecture

> **Detecting Character Backstory Contradictions in 19th-Century Literature Using Biologically-Inspired Neural Networks**

[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.9](https://img.shields.io/badge/PyTorch-2.9+-ee4c2c.svg)](https://pytorch.org/)
[![Pathway 0.27](https://img.shields.io/badge/Pathway-0.27+-green.svg)](https://pathway.com/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

This repository contains our **KDSH 2026 Track B** submission, implementing **TextPath** â€” a novel adaptation of the biologically-inspired [Dragon Hatchling (BDH)](https://arxiv.org/abs/2509.26507) architecture for automated narrative consistency verification in classical literature.

---

## ğŸ“– Table of Contents

1. [Project Overview](#-project-overview)
2. [Architecture](#-architecture)
3. [Project Structure](#-project-structure)
4. [Installation](#-installation)
5. [Usage](#-usage)
6. [Configuration](#-configuration)
7. [Modules Reference](#-modules-reference)
8. [Visualizations](#-visualizations)
9. [Technical Details](#-technical-details)
10. [References](#-references)

---

## ğŸ¯ Project Overview

### The Challenge

**Task**: Given a 19th-century novel and a character backstory, classify whether the backstory is **consistent** with or **contradicts** the original narrative.

**Dataset**: 80 labeled training examples from two novels:
- *The Count of Monte Cristo* by Alexandre Dumas
- *In Search of the Castaways* by Jules Verne

**Challenges**:
| Challenge | Description |
|-----------|-------------|
| ğŸ“– Long-context processing | Novels contain tens of thousands of lines |
| ğŸ­ Complex relationships | Character relationships and plot arcs span entire novels |
| ğŸ” Subtle contradictions | Requires deep narrative understanding |
| âš–ï¸ Plausible vs. impossible | Distinguishing alternate backstories from contradictions |
| ğŸ“Š Limited data | Only 80 examples for binary classification |

### Our Solution: TextPath with BDH Architecture

We employ a **novel-specific pipeline** that leverages the unique biological properties of the Dragon Hatchling (BDH) architecture:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         KDSH Pipeline                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Pathway    â”‚    â”‚  Novel-BDH   â”‚    â”‚   Classification     â”‚   â”‚
â”‚  â”‚   RAG        â”‚ â†’  â”‚  Language    â”‚ â†’  â”‚   Head               â”‚   â”‚
â”‚  â”‚   Retrieval  â”‚    â”‚  Models      â”‚    â”‚   (MLP â†’ Binary)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                     â”‚
â”‚  1. Chunk novels     2. Process with     3. Classify as             â”‚
â”‚     into passages       Hebbian-trained     Consistent/Contradict   â”‚
â”‚     + embed with        BDH model                                   â”‚
â”‚     sentence-transformers                                           â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Key Components

| Component | Purpose | Implementation |
|-----------|---------|----------------|
| **Pathway RAG** | Retrieve relevant passages from novels | `PathwayNovelRetriever` with sentence-transformers embeddings |
| **Novel-Specific BDH** | Learn narrative patterns per novel | Separate pretrained models for each novel |
| **Classification Head** | Binary prediction | MLP on pooled BDH representations |

---

## ğŸ—ï¸ Architecture

### TextPath: BDH-Based Language Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      TextPath Architecture                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  Input: Token IDs [batch_size, seq_len]                              â”‚
â”‚           â†“                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Token Embedding Layer (vocab_size=16384, d_model=256)          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â†“                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ BDH Layers (L=4)                                               â”‚  â”‚
â”‚  â”‚   â”œâ”€ Scale-Free Neuron Network (N=4096 neurons)                â”‚  â”‚
â”‚  â”‚   â”œâ”€ Sparse Activations (~5% active neurons)                   â”‚  â”‚
â”‚  â”‚   â”œâ”€ Multi-Head Attention (H=8 heads)                          â”‚  â”‚
â”‚  â”‚   â””â”€ RoPE Positional Encoding (max_seq_len=4096)               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â†“                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Classification Mode (when enabled):                            â”‚  â”‚
â”‚  â”‚   LayerNorm â†’ Dropout â†’ Linear(256â†’128) â†’ GELU                 â”‚  â”‚
â”‚  â”‚   â†’ Dropout â†’ Linear(128â†’2) â†’ [Contradict, Consistent]         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                      â”‚
â”‚  Output: Logits [batch_size, 2] for classification                   â”‚
â”‚          OR Logits [batch_size, seq_len, vocab_size] for LM          â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### BDH Biological Properties

The Dragon Hatchling architecture provides three key advantages for narrative understanding:

#### 1. Hebbian Learning
> *"Neurons that fire together, wire together"*

Pre-training on sequential novel passages builds causal circuits encoding:
- Character relationships (DantÃ¨s â†’ MercÃ©dÃ¨s, Fernand â†’ betrayal)
- Plot events (imprisonment â†’ escape â†’ revenge)
- Narrative logic (foreshadowing â†’ resolution)

$$\Delta w_{ij} \propto x_i \cdot y_j$$

#### 2. Sparse Activations (~5%)
Each concept (character, location, event) activates distinct neuron groups:
- Creates **monosemantic representations** (one concept per neuron cluster)
- Makes contradictions detectable as conflicting activation patterns
- Enables interpretability of what the model "knows"

$$\|a\|_0 \approx 0.05 \cdot N$$

#### 3. Causal Circuits
Learned connectivity graph encodes "if A then B" reasoning:

$$G_x = E \cdot D_x$$

Where $E$ encodes edge weights and $D_x$ encodes input-dependent dynamics.

### Novel-Specific Routing

The `NovelSpecificClassifier` routes each sample to the appropriate pretrained model:

```python
# Automatic model selection based on book_name
NOVEL_MODEL_MAP = {
    'castaways': 'textpath_in_search_of_the_castaways.pt',
    'monte cristo': 'textpath_the_count_of_monte_cristo.pt',
}
```

This ensures each novel's unique narrative patterns are captured by a dedicated model.

---

## ğŸ“‚ Project Structure

```
KDSH/
â”œâ”€â”€ run_pipeline.py                    # ğŸš€ Main CLI entry point (187 lines)
â”œâ”€â”€ requirements.txt                   # Dependencies (293 packages)
â”œâ”€â”€ results.csv                        # Final predictions for submission
â”œâ”€â”€ LICENSE                            # MIT License
â”‚
â”œâ”€â”€ Dataset/
â”‚   â”œâ”€â”€ train.csv                      # 80 labeled training pairs
â”‚   â”œâ”€â”€ test.csv                       # Unlabeled test set
â”‚   â””â”€â”€ Books/
â”‚       â”œâ”€â”€ The Count of Monte Cristo.txt
â”‚       â””â”€â”€ In search of the castaways.txt
â”‚
â”œâ”€â”€ models/                            # Trained model checkpoints
â”‚   â”œâ”€â”€ custom_tokenizer.json          # 16,384 vocab BPE tokenizer
â”‚   â”œâ”€â”€ textpath_pretrained.pt         # Generic pretrained model
â”‚   â”œâ”€â”€ textpath_the_count_of_monte_cristo.pt    # Monte Cristo BDH
â”‚   â”œâ”€â”€ textpath_in_search_of_the_castaways.pt   # Castaways BDH
â”‚   â”œâ”€â”€ textpath_classifier_best.pt              # Best classifier
â”‚   â”œâ”€â”€ textpath_classifier_best_monte_cristo.pt # Novel-specific
â”‚   â””â”€â”€ textpath_classifier_best_castaways.pt    # Novel-specific
â”‚
â”œâ”€â”€ src/                               # Source code modules
â”‚   â”œâ”€â”€ __init__.py                    # Package exports
â”‚   â”œâ”€â”€ config.py                      # PipelineConfig dataclass
â”‚   â”‚
â”‚   â”œâ”€â”€ data_processing/               # Data and RAG
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ retrieval.py               # PathwayNovelRetriever class
â”‚   â”‚   â”œâ”€â”€ classification_dataset.py # PyTorch Dataset
â”‚   â”‚   â”œâ”€â”€ build_retrievers.py        # Retriever factory functions
â”‚   â”‚   â”œâ”€â”€ ingest.py                  # Data ingestion utilities
â”‚   â”‚   â””â”€â”€ train_tokenizer.py         # BPE tokenizer training
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                        # Neural network modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ textpath.py                # TextPath/BDH core
â”‚   â”‚   â”œâ”€â”€ textpath_classifier.py     # Classifier wrappers
â”‚   â”‚   â”œâ”€â”€ finetune_classifier.py     # train_epoch/validate functions
â”‚   â”‚   â”œâ”€â”€ pretrain_bdh_native.py     # Hebbian pretraining script
â”‚   â”‚   â””â”€â”€ state_manager.py           # Synaptic state management
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                      # Training infrastructure
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ trainer.py                 # Trainer class
â”‚   â”‚   â””â”€â”€ pretraining.py             # Pretraining runner
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/                    # Evaluation and prediction
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ evaluate.py                # Metrics, prediction
â”‚   â”‚   â””â”€â”€ inference.py               # Inference utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ visualization/                 # Analysis and plots
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ visualize.py               # All visualization functions
â”‚   â”‚
â”‚   â””â”€â”€ utils/                         # Helper functions
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ visualizations/                    # Generated plots
â”‚   â”œâ”€â”€ consistency_embedding_space.png
â”‚   â”œâ”€â”€ prediction_confidence.png
â”‚   â”œâ”€â”€ accuracy_by_character.png
â”‚   â””â”€â”€ accuracy_by_book.png
â”‚
â”œâ”€â”€ repos/                             # External dependencies
â”‚   â”œâ”€â”€ bdh_educational/               # Educational BDH implementation
â”‚   â”‚   â”œâ”€â”€ bdh.py                     # Core BDH module
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â””â”€â”€ bdh_official/                  # Official BDH repo reference
â”‚
â”œâ”€â”€ outputs/                           # Training artifacts
â”‚   â”œâ”€â”€ optimal_config.json
â”‚   â”œâ”€â”€ train_predictions.csv
â”‚   â””â”€â”€ tuning_retrieval_k.json
â”‚
â””â”€â”€ logs/                              # Training logs
```

---

## ğŸ”§ Installation

### Prerequisites
- Python 3.11+
- conda (recommended) or pip
- ~8GB RAM (for embedding models)

### Setup

```bash
# Clone the repository
git clone https://github.com/kabyik-kayal/kdsh.git
cd kdsh

# Create conda environment
conda create -n kds python=3.11
conda activate kds

# Install dependencies
pip install -r requirements.txt
```

### Key Dependencies

| Package | Version | Purpose |
|---------|---------|---------|
| `torch` | 2.9.1 | Deep learning framework |
| `pathway` | 0.27.1 | RAG document indexing (Track B requirement) |
| `sentence-transformers` | 5.2.0 | Embedding model for retrieval |
| `tokenizers` | 0.22.2 | BPE tokenizer |
| `scikit-learn` | 1.8.0 | Metrics and evaluation |
| `pandas` | 2.3.3 | Data manipulation |
| `matplotlib` | 3.10.8 | Visualization |
| `tqdm` | 4.67.1 | Progress bars |

---

## ğŸš€ Usage

### Quick Start

```bash
# Run the complete pipeline (train â†’ evaluate â†’ predict)
python run_pipeline.py --mode full
```

### Pipeline Modes

| Mode | Description | Command |
|------|-------------|---------|
| `pretrain` | Hebbian pretraining on novel texts | `python run_pipeline.py --mode pretrain` |
| `train` | Train classification head | `python run_pipeline.py --mode train` |
| `evaluate` | Evaluate on validation split | `python run_pipeline.py --mode evaluate` |
| `predict` | Generate test predictions | `python run_pipeline.py --mode predict` |
| `full` | Train + Evaluate + Predict | `python run_pipeline.py --mode full` |

### Command-Line Options

```bash
python run_pipeline.py --help

options:
  --mode {pretrain,train,predict,evaluate,full}
                        Pipeline mode (default: full)
  --pretrain-epochs PRETRAIN_EPOCHS
                        BDH pretraining epochs (default: 50)
  --epochs EPOCHS       Classifier training epochs (default: 15)
  --batch-size BATCH_SIZE
                        Training batch size (default: 4)
  --lr LR               Learning rate (default: 1e-4)
```

### Examples

```bash
# Pretrain BDH models for 100 epochs
python run_pipeline.py --mode pretrain --pretrain-epochs 100

# Train classifier for 20 epochs with larger batch
python run_pipeline.py --mode train --epochs 20 --batch-size 8

# Just generate predictions (requires trained model)
python run_pipeline.py --mode predict

# Generate visualizations
python src/visualization/visualize.py
```

---

## âš™ï¸ Configuration

All configuration is centralized in `src/config.py` via the `PipelineConfig` dataclass:

### Paths

| Parameter | Default | Description |
|-----------|---------|-------------|
| `novels_dir` | `Dataset/Books/` | Directory containing novel .txt files |
| `train_csv` | `Dataset/train.csv` | Training data CSV |
| `test_csv` | `Dataset/test.csv` | Test data CSV |
| `tokenizer_path` | `models/custom_tokenizer.json` | BPE tokenizer |
| `models_dir` | `models/` | Directory for checkpoints |
| `output_model` | `models/textpath_classifier_best.pt` | Best model path |
| `output_predictions` | `results.csv` | Predictions output |

### Training Hyperparameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `batch_size` | 4 | Training batch size |
| `epochs` | 15 | Classifier training epochs |
| `learning_rate` | 1e-4 | Initial learning rate |
| `weight_decay` | 0.01 | AdamW weight decay |
| `max_tokens` | 512 | Maximum sequence length |

### Freezing Strategy

| Parameter | Default | Description |
|-----------|---------|-------------|
| `freeze_bdh` | True | Freeze BDH layers initially |
| `unfreeze_after_epoch` | 5 | Epoch to unfreeze BDH |
| `unfreeze_lr_multiplier` | 0.1 | LR multiplier after unfreezing |

### RAG Settings

| Parameter | Default | Description |
|-----------|---------|-------------|
| `chunk_size` | 200 | Words per chunk (~250 tokens) |
| `overlap` | 50 | Overlapping words between chunks |
| `top_k_retrieval` | 2 | Number of passages to retrieve |

### Class Weights

| Parameter | Default | Description |
|-----------|---------|-------------|
| `class_weight_inconsistent` | 1.7 | Weight for "Contradict" class |
| `class_weight_consistent` | 1.0 | Weight for "Consistent" class |

### Device

Automatically detected in order: CUDA â†’ MPS (Apple Silicon) â†’ CPU

---

## ğŸ“Š Visualizations

The pipeline generates analysis plots in `visualizations/`:

| Plot | Description |
|------|-------------|
| `consistency_embedding_space.png` | 2D t-SNE projection showing consistent vs contradictory samples |
| `prediction_confidence.png` | Distribution of model confidence (entropy) across predictions |
| `accuracy_by_character.png` | Per-character classification accuracy |
| `accuracy_by_book.png` | Per-novel classification accuracy |

### Generate Visualizations

```bash
python src/visualization/visualize.py
```

This analyzes the trained classifier and generates all plots.

---

## ğŸ”¬ Technical Details

### TextPath Configuration

```python
@dataclass
class TextPathConfig:
    vocab_size: int = 16384     # Custom BPE tokenizer vocabulary
    max_seq_len: int = 4096     # Maximum sequence length
    n_heads: int = 8            # Attention heads
    n_neurons: int = 4096       # BDH neurons (scale-free graph)
    d_model: int = 256          # Model embedding dimension
    n_layers: int = 4           # Number of BDH layers
    dropout: float = 0.1        # Dropout rate
    use_rope: bool = True       # Rotary position encoding
    sparsity_target: float = 0.05  # 5% neuron activation target
    classification_mode: bool = False  # Enable classification head
```

### Training Strategy

1. **Phase 1: BDH Frozen** (epochs 1-5)
   - Only train classification head
   - Learning rate: 1e-4
   - Preserves pretrained narrative knowledge

2. **Phase 2: Full Fine-tuning** (epochs 6-15)
   - Unfreeze BDH layers
   - Reduced learning rate: 1e-5 (0.1Ã— multiplier)
   - Gentle adaptation to classification task

3. **Optimizer**: AdamW with weight decay 0.01
4. **Scheduler**: Cosine annealing over total epochs
5. **Class Weights**: [1.7, 1.0] to handle imbalance (~36% contradict, ~64% consistent)

### Pathway Integration

```python
# Creating Pathway table from chunks
self.chunks_table = pw.debug.table_from_rows(
    schema=pw.schema_from_dict({"text": str}),
    rows=[(chunk,) for chunk in self.chunks]
)

# Embedding with sentence-transformers via Pathway
from pathway.xpacks import llm
self.embedder = llm.embedders.SentenceTransformerEmbedder(
    model="sentence-transformers/all-MiniLM-L6-v2"
)
```

### Model Files

| File | Size | Description |
|------|------|-------------|
| `textpath_the_count_of_monte_cristo.pt` | ~50MB | Monte Cristo pretrained BDH |
| `textpath_in_search_of_the_castaways.pt` | ~50MB | Castaways pretrained BDH |
| `textpath_classifier_best.pt` | ~55MB | Best classifier checkpoint |
| `custom_tokenizer.json` | ~2MB | 16,384 vocab BPE tokenizer |

---

## ğŸ“š References

- **Dragon Hatchling (BDH)**: [arXiv:2509.26507](https://arxiv.org/abs/2509.26507) - Kosowski et al. (2025)
- **Pathway**: [pathway.com](https://pathway.com/) - Real-time data processing framework
- **sentence-transformers**: [SBERT.net](https://www.sbert.net/) - Sentence embeddings
- **Rotary Position Embedding (RoPE)**: [arXiv:2104.09864](https://arxiv.org/abs/2104.09864)

---